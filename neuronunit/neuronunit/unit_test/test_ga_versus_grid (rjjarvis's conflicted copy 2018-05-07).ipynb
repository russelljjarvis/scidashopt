{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Assumptions about this code:\n",
    "The NB was launched with a command that mounts two volumes inside a docker container. \n",
    "In the future invocation of this script will be simplified greatly. NU is from a specific fork and branch -b results https://github.com/russelljjarvis/neuronunit \n",
    "BluePyOpt is also from a specific fork and branch: -b elitism https://github.com/russelljjarvis/BluePyOpt\n",
    "\n",
    "Below BASH code for Ubuntu host:\n",
    "\n",
    "``` bash\n",
    "cd ~/git/neuronunit; sudo docker run -it -v `pwd`:/home/jovyan/neuronunit -v ~/git/BluePyOpt:/home/jovyan/BluePyOpt neuronunit-optimization /bin/bash'\n",
    "```\n",
    "\n",
    "## Parallel Environment.\n",
    "Parallelisation module: dask distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ign http://cdn-fastly.deb.debian.org jessie InRelease\n",
      "Hit http://cdn-fastly.deb.debian.org jessie/updates InRelease\n",
      "Hit http://cdn-fastly.deb.debian.org jessie Release.gpg\n",
      "Hit http://cdn-fastly.deb.debian.org jessie Release\n",
      "Get:1 http://cdn-fastly.deb.debian.org jessie/updates/main amd64 Packages [650 kB]\n",
      "Get:2 http://cdn-fastly.deb.debian.org jessie/main amd64 Packages [9,064 kB]\n",
      "Fetched 9,714 kB in 6s (1,471 kB/s)                                            \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "graphviz is already the newest version.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 177 not upgraded.\n",
      "Fetching package metadata ...........\n",
      "Solving package specifications: ..........\n",
      "\n",
      "Package plan for installation in environment /opt/conda:\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    futures-compat-1.0         |            py3_0          313 B\n",
      "    pandas-0.22.0              |           py35_1        25.6 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        25.6 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "    futures-compat: 1.0-py3_0                     \n",
      "    packaging:      17.1-py_0          conda-forge\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    bokeh:          0.11.1-py35_0                  --> 0.12.15-pyh8da4227_0 bokeh      \n",
      "    pandas:         0.19.2-np110py35_1 conda-forge --> 0.22.0-py35_1        conda-forge\n",
      "    yaml:           0.1.6-0                        --> 0.1.7-0              conda-forge\n",
      "\n",
      "Fetching packages ...\n",
      "futures-compat 100% |################################| Time: 0:00:00 271.75 kB/s\n",
      "pandas-0.22.0- 100% |################################| Time: 0:00:11   2.38 MB/s\n",
      "Extracting packages ...\n",
      "[      COMPLETE      ]|###################################################| 100%\n",
      "Unlinking packages ...\n",
      "[      COMPLETE      ]|###################################################| 100%\n",
      "Linking packages ...\n",
      "[pandas              ]|########################################           |  80%\n",
      "\n",
      "CondaOSError: OS error: failed to link (src='/opt/conda/pkgs/pandas-0.22.0-py35_1/lib/python3.5/site-packages/pandas/api/__pycache__/__init__.cpython-35.pyc', dst='/opt/conda/lib/python3.5/site-packages/pandas/api/__pycache__/__init__.cpython-35.pyc', type=1, error=FileExistsError(17, 'File exists'))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.system('jupyter trust test_ga_versus_grid.ipynb'); #suppress the untrusted notebook warning.\n",
    "os.system('sudo apt-get install -y graphviz')\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y graphviz\n",
    "!conda install -y bokeh -c bokeh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling sciunit-0.19:\n",
      "  Successfully uninstalled sciunit-0.19\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Uninstalling dask-0.16.0:\n",
      "  Successfully uninstalled dask-0.16.0\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "/bin/sh: 1: multiprocessing: not found\n",
      "Collecting git+https://github.com/scidash/sciunit@dev\n",
      "  Cloning https://github.com/scidash/sciunit (to dev) to /tmp/pip-k1unhiwq-build\n",
      "Requirement already satisfied: cypy>=0.2 in /opt/conda/lib/python3.5/site-packages (from sciunit==0.19)\n",
      "Requirement already satisfied: quantities==0.12.1 in /opt/conda/lib/python3.5/site-packages (from sciunit==0.19)\n",
      "Requirement already satisfied: pandas>=0.18 in /opt/conda/lib/python3.5/site-packages (from sciunit==0.19)\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.5/site-packages (from sciunit==0.19)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.5/site-packages (from sciunit==0.19)\n",
      "Requirement already satisfied: bs4 in /opt/conda/lib/python3.5/site-packages (from sciunit==0.19)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.5/site-packages (from sciunit==0.19)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.5/site-packages (from sciunit==0.19)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.5/site-packages (from sciunit==0.19)\n",
      "Requirement already satisfied: nbformat in /opt/conda/lib/python3.5/site-packages (from sciunit==0.19)\n",
      "Requirement already satisfied: gitpython in /opt/conda/lib/python3.5/site-packages (from sciunit==0.19)\n",
      "Requirement already satisfied: python-dateutil>=2 in /opt/conda/lib/python3.5/site-packages (from pandas>=0.18->sciunit==0.19)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.5/site-packages (from pandas>=0.18->sciunit==0.19)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /opt/conda/lib/python3.5/site-packages (from pandas>=0.18->sciunit==0.19)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.5/site-packages (from ipython->sciunit==0.19)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.5/site-packages (from ipython->sciunit==0.19)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /opt/conda/lib/python3.5/site-packages (from ipython->sciunit==0.19)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.5/site-packages (from ipython->sciunit==0.19)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /opt/conda/lib/python3.5/site-packages (from ipython->sciunit==0.19)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.5/site-packages (from ipython->sciunit==0.19)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.5/site-packages (from ipython->sciunit==0.19)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.5/site-packages (from ipython->sciunit==0.19)\n",
      "Requirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.5/site-packages (from ipython->sciunit==0.19)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.5/site-packages (from matplotlib->sciunit==0.19)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.5/site-packages (from matplotlib->sciunit==0.19)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.5/site-packages (from matplotlib->sciunit==0.19)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.5/site-packages (from bs4->sciunit==0.19)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.5/site-packages (from nbconvert->sciunit==0.19)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.5/site-packages (from nbconvert->sciunit==0.19)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.5/site-packages (from nbconvert->sciunit==0.19)\n",
      "Requirement already satisfied: mistune>=0.7.4 in /opt/conda/lib/python3.5/site-packages (from nbconvert->sciunit==0.19)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /opt/conda/lib/python3.5/site-packages (from nbconvert->sciunit==0.19)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.5/site-packages (from nbconvert->sciunit==0.19)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.5/site-packages (from nbconvert->sciunit==0.19)\n",
      "Requirement already satisfied: jupyter-client in /opt/conda/lib/python3.5/site-packages (from ipykernel->sciunit==0.19)\n",
      "Requirement already satisfied: tornado>=4.0 in /opt/conda/lib/python3.5/site-packages (from ipykernel->sciunit==0.19)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.5/site-packages (from nbformat->sciunit==0.19)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.5/site-packages (from nbformat->sciunit==0.19)\n",
      "Requirement already satisfied: gitdb2>=2.0.0 in /opt/conda/lib/python3.5/site-packages (from gitpython->sciunit==0.19)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.5/site-packages (from pexpect; sys_platform != \"win32\"->ipython->sciunit==0.19)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.5/site-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->sciunit==0.19)\n",
      "Requirement already satisfied: parso==0.1.1 in /opt/conda/lib/python3.5/site-packages (from jedi>=0.10->ipython->sciunit==0.19)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.5/site-packages (from jinja2->nbconvert->sciunit==0.19)\n",
      "Requirement already satisfied: html5lib!=0.9999,!=0.99999,<0.99999999,>=0.999 in /opt/conda/lib/python3.5/site-packages (from bleach->nbconvert->sciunit==0.19)\n",
      "Requirement already satisfied: pytest; python_version >= \"3.4\" or python_version == \"2.7\" and extra == \"test\" in /opt/conda/lib/python3.5/site-packages (from jupyter-client->ipykernel->sciunit==0.19)\n",
      "Requirement already satisfied: pyzmq>=13 in /opt/conda/lib/python3.5/site-packages (from jupyter-client->ipykernel->sciunit==0.19)\n",
      "Requirement already satisfied: smmap2>=2.0.0 in /opt/conda/lib/python3.5/site-packages (from gitdb2>=2.0.0->gitpython->sciunit==0.19)\n",
      "Requirement already satisfied: py>=1.5.0 in /opt/conda/lib/python3.5/site-packages (from pytest; python_version >= \"3.4\" or python_version == \"2.7\" and extra == \"test\"->jupyter-client->ipykernel->sciunit==0.19)\n",
      "Requirement already satisfied: pluggy<0.7,>=0.5 in /opt/conda/lib/python3.5/site-packages (from pytest; python_version >= \"3.4\" or python_version == \"2.7\" and extra == \"test\"->jupyter-client->ipykernel->sciunit==0.19)\n",
      "Requirement already satisfied: attrs>=17.2.0 in /opt/conda/lib/python3.5/site-packages (from pytest; python_version >= \"3.4\" or python_version == \"2.7\" and extra == \"test\"->jupyter-client->ipykernel->sciunit==0.19)\n",
      "Installing collected packages: sciunit\n",
      "  Found existing installation: sciunit 0.1.5.8\n",
      "    Uninstalling sciunit-0.1.5.8:\n",
      "      Successfully uninstalled sciunit-0.1.5.8\n",
      "  Running setup.py install for sciunit ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25hSuccessfully installed sciunit-0.19\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting graphviz\n",
      "  Using cached https://files.pythonhosted.org/packages/84/44/21a7fdd50841aaaef224b943f7d10df87e476e181bb926ccf859bcb53d48/graphviz-0.8.3-py2.py3-none-any.whl\n",
      "Installing collected packages: graphviz\n",
      "Successfully installed graphviz-0.8.3\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting dask\n",
      "  Using cached https://files.pythonhosted.org/packages/fc/64/b0176ff8baa9ea3402711f9f03942f67de9b9518e9be19ec2b0919f8b9c5/dask-0.17.4-py2.py3-none-any.whl\n",
      "Installing collected packages: dask\n",
      "Successfully installed dask-0.17.4\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "!pip uninstall sciunit -y\n",
    "!pip uninstall dask -y\n",
    "!multiprocessing\n",
    "!pip install git+https://github.com/scidash/sciunit@dev\n",
    "!pip install graphviz \n",
    "!pip install dask "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################\n",
    "# GA parameters:\n",
    "about $10^{3}=30$ models will be made, excluding rheobase search.\n",
    "################\n",
    "\n",
    "\n",
    "# Choice of selection criteria is important. \n",
    "Here we use BluepyOpts IBEA, such that it can be compared to NSGA2.\n",
    "\n",
    "https://link.springer.com/article/10.1007/s00500-005-0027-5\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MU = 6; NGEN = 6; CXPB = 0.9\n",
    "USE_CACHED_GA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '/home/jovyan/mnt/neuronunit/neuronunit/optimization',\n",
       " '/opt/conda/lib/python35.zip',\n",
       " '/opt/conda/lib/python3.5',\n",
       " '/opt/conda/lib/python3.5/plat-linux',\n",
       " '/opt/conda/lib/python3.5/lib-dynload',\n",
       " '/opt/conda/lib/python3.5/site-packages',\n",
       " '/opt/conda/lib/python3.5/site-packages/cycler-0.10.0-py3.5.egg',\n",
       " '/opt/conda/lib/python3.5/site-packages/sciunit-0.1.5.8-py3.5.egg',\n",
       " '/opt/conda/lib/python3.5/site-packages/elephant-0.4.1-py3.5.egg',\n",
       " '/opt/conda/lib/python3.5/site-packages/neo-0.4.0-py3.5.egg',\n",
       " '/opt/conda/lib/python3.5/site-packages/argparse-1.1-py3.5.egg',\n",
       " '/opt/conda/lib/python3.5/site-packages/greenlet-0.4.12-py3.5-linux-x86_64.egg',\n",
       " '/opt/conda/lib/python3.5/site-packages/libNeuroML-0.2.30-py3.5.egg',\n",
       " '/opt/conda/lib/python3.5/site-packages/airspeed-0.5.4.dev20150515-py3.5.egg',\n",
       " '/opt/conda/lib/python3.5/site-packages/PyLEMS-0.4.9-py3.5.egg',\n",
       " '/opt/conda/lib/python3.5/site-packages/lxml-3.8.0-py3.5-linux-x86_64.egg',\n",
       " '/opt/conda/lib/python3.5/site-packages/requests_toolbelt-0.8.0-py3.5.egg',\n",
       " '/opt/conda/lib/python3.5/site-packages/pynrrd-0.2.1-py3.5.egg',\n",
       " '/opt/conda/lib/python3.5/site-packages/bs4-0.0.1-py3.5.egg',\n",
       " '/opt/conda/lib/python3.5/site-packages/quantities-0.11.1-py3.5.egg',\n",
       " '/opt/conda/lib/python3.5/site-packages/cypy-0.2.0-py3.5.egg',\n",
       " '/opt/conda/lib/python3.5/site-packages/cachetools-0.8.0-py3.5.egg',\n",
       " '/opt/conda/lib/python3.5/site-packages/setuptools-27.2.0-py3.5.egg',\n",
       " '/opt/conda/lib/python3.5/site-packages/gevent-1.2.2-py3.5-linux-x86_64.egg',\n",
       " '/opt/conda/lib/python3.5/site-packages/scoop-0.7.2.0-py3.5.egg',\n",
       " '/opt/conda/lib/python3.5/site-packages/future-0.16.0-py3.5.egg',\n",
       " '/opt/conda/lib/python3.5/site-packages/efel-2.12.6-py3.5-linux-x86_64.egg',\n",
       " '/opt/conda/lib/python3.5/site-packages/deap-1.0.2-py3.5.egg',\n",
       " '/home/jovyan/BluePyOpt',\n",
       " '/home/jovyan/neuronunit',\n",
       " '/opt/conda/lib/python3.5/site-packages/IPython/extensions',\n",
       " '/home/jovyan/.ipython']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!sudo /opt/conda/bin/pip install -e ../../\n",
    "#import neuronunit\n",
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################\n",
    "# Grid search parameters:\n",
    "$ 2^{10}=1024 $ models, will be made excluding rheobase search\n",
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuronUnit requires SciUnit: http://github.com/scidash/sciunit\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'pandas.core'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ad592f976c68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mneuronunit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_parameters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprovided_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mUSE_CACHED_GS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/neuronunit/neuronunit/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NeuronUnit requires SciUnit: http://github.com/scidash/sciunit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mIMPLEMENTATION\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplatform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_implementation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/neuronunit/neuronunit/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0msciunit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NeuronUnit requires SciUnit: http://github.com/scidash/sciunit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/sciunit/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcapabilities\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCapability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/sciunit/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msciunit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msciunit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSciUnit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtkinter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPYTHON_MAJOR_VERSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0munittest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/sciunit/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGitCommandError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# let init-time option registration happen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'pandas.core'"
     ]
    }
   ],
   "source": [
    "npoints = 2\n",
    "nparams = 10\n",
    "from neuronunit.optimization.model_parameters import model_params\n",
    "provided_keys = list(model_params.keys())\n",
    "USE_CACHED_GS = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An oppurtunity to improve grid search, by increasing resolution of search intervals given a first pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "REFINE_GRID = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from distributed import Client\n",
    "client = Client()\n",
    "dask.set_options(scheduler='single-threaded')\n",
    "print(client.scheduler_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neuronunit.optimization import get_neab #import get_neuron_criteria, impute_criteria\n",
    "fi_basket = {'nlex_id':'NLXCELL:100201'}\n",
    "pvis_cortex = {'nlex_id': 'nifext_50'} # Layer V pyramidal cell\n",
    "\n",
    "try: \n",
    "    #assert 1==2\n",
    "    contents = pickle.load(open('ne_neuron_criteria.p','rb'))\n",
    "    pvis_criterion, inh_criterion = contents\n",
    "    #print(inh_criterion, inh_observations)\n",
    "\n",
    "\n",
    "except:\n",
    "    pvis_criterion, pvis_observations = get_neab.get_neuron_criteria(pvis_cortex)\n",
    "    inh_criterion, inh_observations = get_neab.get_neuron_criteria(fi_basket)\n",
    "    #print(type(inh_observations),inh_observations)\n",
    "\n",
    "    inh_observations = get_neab.impute_criteria(pvis_observations,inh_observations)\n",
    "\n",
    "    inh_criterion, inh_observations = get_neab.get_neuron_criteria(fi_basket,observation = inh_observations)\n",
    "    with open('ne_neuron_criteria.p','wb') as f:\n",
    "       pickle.dump((pvis_criterion, inh_criterion),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if not USE_CACHED_GA:\n",
    "\n",
    "    from neuronunit.optimization import get_neab#.get_neab import get_neuron_criteria, impute_criteria\n",
    "    fi_basket = {'nlex_id':'NLXCELL:100201'}\n",
    "    pvis_cortex = {'nlex_id': 'nifext_50'} # Layer V pyramidal cell\n",
    "    try:\n",
    "        #assert 1==2\n",
    "        contents = pickle.load(open('ne_neuron_criteria.p','rb'))\n",
    "        pvis_criterion, inh_criterion = contents\n",
    "    except:\n",
    "        pvis_criterion, pvis_observations = get_neab.get_neuron_criteria(pvis_cortex)\n",
    "        inh_criterion, inh_observations = get_neab.get_neuron_criteria(fi_basket)\n",
    "        inh_observations = get_neab.impute_criteria(pvis_observations,inh_observations)\n",
    "        inh_criterion, inh_observations = get_neab.get_neuron_criteria(fi_basket,observation = inh_observations)\n",
    "    \n",
    "        with open('ne_neuron_criteria.p','wb') as f:\n",
    "           pickle.dump((pvis_criterion, inh_criterion),f)    \n",
    "        '''    \n",
    "        with open('ga_dumpnifext_50.p','rb') as f:\n",
    "           [pop_py, log_py, history_py, _hof_py, td_py] = pickle.load(f)\n",
    "        with open('ga_dump_NLXCELL:100201.p','rb') as f:\n",
    "            in_results = pickle.load(f)\n",
    "            if len(in_results) == 7:\n",
    "                hof_fi = in_results['halloffame']\n",
    "            else:\n",
    "               [pop_fi, log_fi, history_fi, _hof_fi, td_fi] = in_results\n",
    "        '''\n",
    "        #else:\n",
    "\n",
    "\n",
    "        from bluepyopt.deapext.optimisations import DEAPOptimisation\n",
    "        DO = DEAPOptimisation(error_criterion = pvis_criterion, selection = 'selIBEA')\n",
    "        DO.setnparams(nparams = nparams, provided_keys = provided_keys)\n",
    "        pop, hof_py, log, history, td_py, gen_vs_hof = DO.run(offspring_size = MU, max_ngen = NGEN, cp_frequency=0,cp_filename='ga_dumpnifext_50.p')\n",
    "\n",
    "        with open('ga_dump_NLXCELL:100201.p','wb') as f:\n",
    "           pickle.dump([pop, log, history, hof_py, td_py],f)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#with open('ga_dumpnifext_50.p','wb') as f:\n",
    "#   pickle.dump([pop, log, history, hof_py, td_py],f)\n",
    "#\n",
    "from dask import visualize\n",
    "#visualize(*lazy_dataframes[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CACHED_GA = False\n",
    "import pickle\n",
    "npoints = 2\n",
    "nparams = 10\n",
    "from neuronunit.optimization.model_parameters import model_params\n",
    "provided_keys = list(model_params.keys())\n",
    "USE_CACHED_GS = False\n",
    "if not USE_CACHED_GA:\n",
    "    \n",
    "    try:\n",
    "        contents = pickle.load(open('ne_neuron_criteria.p','rb'))\n",
    "        pvis_criterion, inh_criterion = contents\n",
    "    except:\n",
    "        pvis_criterion, pvis_observations = get_neab.get_neuron_criteria(pvis_cortex)\n",
    "        inh_criterion, inh_observations = get_neab.get_neuron_criteria(fi_basket)\n",
    "        inh_observations = get_neab.impute_criteria(pvis_observations,inh_observations)\n",
    "        inh_criterion, inh_observations = get_neab.get_neuron_criteria(fi_basket,observation = inh_observations)\n",
    "\n",
    "        with open('ne_neuron_criteria.p','wb') as f:\n",
    "           pickle.dump((pvis_criterion, inh_criterion),f)   \n",
    "    DO = None\n",
    "    pop = None \n",
    "    hof_py = None\n",
    "    history = None\n",
    "    log = None\n",
    "    td = None\n",
    "    from bluepyopt.deapext.optimisations import DEAPOptimisation\n",
    "    DO = DEAPOptimisation(error_criterion = inh_criterion, selection = 'selIBEA')\n",
    "    DO.setnparams(nparams = nparams, provided_keys = provided_keys)\n",
    "    pop, hof_fi, log, history, td_fi, gen_vs_hof = DO.run(offspring_size = MU, max_ngen = NGEN, cp_frequency=0,cp_filename='ga_dump_NLXCELL:100201.p')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('ga_dump_NLXCELL:100201.p','wb') as f:\n",
    "   pickle.dump([pop, hof_fi, log, history, td_hof, gen_vs_hof],f)\n",
    "opt_fi = { k:hof_fi[v] for v,k in enumerate(td_fi) }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#del pop\n",
    "dir()\n",
    "dask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#hof_py = _hof_py[0]\n",
    "#hof_fi = _hof_fi[0]\n",
    "opt_py = { k:hof_py[v] for v,k in enumerate(td_py) }\n",
    "print(opt_fi)\n",
    "#print(opt_py)\n",
    "\n",
    "for k,v in opt_py.items():\n",
    "    print('param',k,v,opt_fi[k],'delta',abs(v-opt_fi[k]))\n",
    "from neuronunit.optimization.optimization_management import write_opt_to_nml\n",
    "fname = 'nifext_50'\n",
    "write_opt_to_nml(fname, opt_py)\n",
    "fname = 'NLXCELL_100201'\n",
    "write_opt_to_nml(fname, opt_fi)\n",
    "\n",
    "\n",
    "'''\n",
    "Switching to the terminal shows:\n",
    "    Starting simulation in NEURON of 650.0ms generated from NeuroML2 model...\n",
    "\n",
    "Population RS_pop contains 1 instance(s) of component: RS of type: izhikevich2007Cell\n",
    "score Z = -inf\n",
    "<class 'float'> type sort_key\n",
    "InjectedCurrentAPAmplitudeTest 0.0\n",
    "rpe = list(update_deap_pop([_hof_py[0],_hof_py[1]],pvis_criterion,td_py))\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neuronunit.optimization.optimization_management import update_deap_pop\n",
    "rpi = list(update_deap_pop([hof[0],hof[1]],inh_criterion,td))\n",
    "\n",
    "\n",
    "dtcpopi = rpi[0][0]\n",
    "print(dtcpopi.scores)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "sci = pd.DataFrame(list(dtcpopi.scores.values()),columns = list(dtcpopi.scores.keys()))\n",
    "print(sci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#dtcpopi.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtcpope = rpe[0][0]\n",
    "import pandas as pd\n",
    "\n",
    "sce = pd.DataFrame(dtcpope.scores.values(),columns=dtcpope.scores.keys())\n",
    "sce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neuronunit.plottools import plot_surface\n",
    "pop,log,history,hof,td = [pop_fi, log_fi, history_fi, _hof_fi, td_fi]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below two error surface slices from the hypervolume are plotted.\n",
    "The data that is plotted consists of the error as experienced by the GA.\n",
    "Note: the GA performs an incomplete, and efficient sampling of the parameter space, and therefore sample points are irregularly spaced. Polygon interpolation is used to visualize error gradients. Existing plotting code from the package BluePyOpt has been extended for this purpose.\n",
    "Light blue dots indicate local minima's of error experienced by the NSGA algrorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_surface('a','b',td,history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plot_surface('v0','vt',td,history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "fig, axes = plt.subplots(figsize=(10, 10), facecolor='white')\n",
    "gen_numbers =[ i for i in range(0,len(log.select('gen'))) ]\n",
    "mean = np.array([ np.sqrt(np.mean(np.square(i))) for i in log.select('avg')])\n",
    "std = np.array([ np.sqrt(np.mean(np.square(i))) for i in log.select('std')])\n",
    "minimum = np.array([ np.sqrt(np.mean(np.square(i))) for i in log.select('min')])\n",
    "best_line = np.array([ np.sqrt(np.mean(np.square(list(p.fitness.values))))  for p in hof])\n",
    "blg = [ best_line[h] for i, h in enumerate(gen_numbers) ]\n",
    "\n",
    "\n",
    "\n",
    "stdminus = mean - std\n",
    "stdplus = mean + std\n",
    "try:\n",
    "    assert len(gen_numbers) == len(stdminus) == len(stdplus)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "axes.plot(\n",
    "    gen_numbers,\n",
    "    mean,\n",
    "    color='black',\n",
    "    linewidth=2,\n",
    "    label='population average')\n",
    "axes.fill_between(gen_numbers, stdminus, stdplus)\n",
    "axes.plot(gen_numbers, blg,'y--', linewidth=2,  label='grid search error')\n",
    "#axes.plot(gen_numbers, bl, 'go', linewidth=2, label='hall of fame error')\n",
    "\n",
    "axes.plot(gen_numbers, stdminus, label='std variation lower limit')\n",
    "axes.plot(gen_numbers, stdplus, label='std variation upper limit')\n",
    "\n",
    "axes.set_xlim(np.min(gen_numbers) - 1, np.max(gen_numbers) + 1)\n",
    "axes.set_xlabel('Generations')\n",
    "axes.set_ylabel('Sum of objectives')\n",
    "axes.legend()\n",
    "fig.tight_layout()\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment on plot\n",
    "The plot shows the mean error value of the population as the GA evolves it's population. The red interval at any instant is the standard deviation of the error. The fact that the mean GA error is able to have a net upwards trajectory, after experiencing a temporary downwards trajectory, demonstrates that the GA retains a drive to explore, and is resiliant against being stuck in a local minima. Also in the above plot population variance in error stays remarkably constant, in this way BluePyOpts selection criteria SELIBEA contrasts with DEAPs native selection strategy NSGA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if USE_CACHED_GS:\n",
    "    import pickle\n",
    "    # shelve cache \n",
    "    first_third = pickle.load(open('/home/jovyan/neuronunit/neuronunit/unit_test/grid_dump_first_3rd.p','rb'))\n",
    "    second_third = pickle.load(open('/home/jovyan/neuronunit/neuronunit/unit_test/grid_dump_second_3rd.p','rb'))\n",
    "    final_third = pickle.load(open('/home/jovyan/neuronunit/neuronunit/unit_test/grid_dump_final_3rd.p','rb'))\n",
    "\n",
    "    second_third.extend(first_third)\n",
    "    second_third.extend(final_third)\n",
    "    dtcpopg = second_third\n",
    "\n",
    "else:\n",
    "    from neuronunit.optimization import exhaustive_search\n",
    "    from neuronunit.optimization import get_neab\n",
    "    from neuronunit.optimization import optimization_management\n",
    "\n",
    "    import dask.bag as db\n",
    "    grid_points = exhaustive_search.create_grid(npoints = npoints,nparams = nparams)\n",
    "    b0 = db.from_sequence(grid_points, npartitions=8)\n",
    "    dlist = list(db.map(optimization_management.update_dtc_pop,b0).compute())\n",
    "    for d in dlist:\n",
    "        d.model_path = get_neab.LEMS_MODEL_PATH\n",
    "        d.LEMS_MODEL_PATH = get_neab.LEMS_MODEL_PATH\n",
    "    # this is a big load on memory so divide it into thirds.\n",
    "\n",
    "    dlist_first_third = dlist[0:int(len(dlist)/N)]\n",
    "    dlist_second_third = dlist[int(len(dlist)/N):int(2*len(dlist)/N)]\n",
    "    dlist_final_third = dlist[int(2*len(dlist)/N):-1]\n",
    "    from neuronunit.optimization.exhaustive_search import dtc_to_rheo\n",
    "    from neuronunit.optimization.nsga_parallel import nunit_evaluate\n",
    "\n",
    "\n",
    "    def compute_chunk(dlist_half):\n",
    "        dlist_half = list(map(dtc_to_rheo,dlist_half))\n",
    "        dlist_half = (nunit_evaluate,dlist_half)\n",
    "        return dlist_half\n",
    "\n",
    "    dlist_first_3rd = compute_chunk(dlist_first_third)\n",
    "    import pickle\n",
    "    with open('grid_dump_first_3rd.p','wb') as f:\n",
    "       pickle.dump(dlist_first_3rd,f)\n",
    "    # Garbage collect a big memory burden.\n",
    "    dlist_first_3rd = None\n",
    "    dlist_second_3rd = compute_chunk(dlist_second_third)\n",
    "\n",
    "    with open('grid_dump_second_3rd.p','wb') as f:\n",
    "       pickle.dump(dlist_second_3rd,f)\n",
    "    # Garbage collect a big memory burden.\n",
    "    dlist_second_3rd = None\n",
    "\n",
    "    dlist_final_3rd = compute_chunk(dlist_final_third)\n",
    "    with open('grid_dump_final_3rd.p','wb') as f:\n",
    "       pickle.dump(dlist_final_3rd,f)\n",
    "    # Garbage collect a big memory burden.\n",
    "    dlist_final_3rd = None\n",
    "    first_third = pickle.load(open('grid_dump_first_3rd.p','rb'))\n",
    "    second_third = pickle.load(open('grid_dump_second_3rd.p','rb'))\n",
    "    final_third = pickle.load(open('grid_dump_final_3rd.p','rb'))\n",
    "\n",
    "    second_third.extend(first_third)\n",
    "    second_third.extend(final_third)\n",
    "    dtcpopg = second_third\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtcpopg = [ dtc for dtc in dtcpopg if not None in (dtc.scores.values()) ]\n",
    "dtcpopg = [ (dtc,sum(list(dtc.scores.values()))) for dtc in dtcpopg ]\n",
    "\n",
    "\n",
    "sorted_grid = sorted(dtcpopg,key=lambda x:x[1])\n",
    "sorted_grid = [dtc[0] for dtc in sorted_grid]\n",
    "#print(sorted_grid)\n",
    "mini = dtcpopg[0][1]\n",
    "maxi = dtcpopg[-1][1]\n",
    "minimagr = sorted_grid[0]\n",
    "minimagr_dtc = sorted_grid[0]\n",
    "minimagr_dtc_1 = sorted_grid[1]\n",
    "minimagr_dtc_2 = sorted_grid[2]\n",
    "from neuronunit.optimization.exhaustive_search import create_refined_grid\n",
    "refined_grid = create_refined_grid(minimagr_dtc, minimagr_dtc_1,minimagr_dtc_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(td)\n",
    "def pop2dtc(pop1,DO,td):\n",
    "    '''\n",
    "    This function takes the DEAP population data type, and converts it to a more convenient\n",
    "    data transport object, which can more readily be used in plotting functions.\n",
    "    This a wasteful, recompute, which is in part necessitated because\n",
    "    deaps pareto front object, only returns gene individual objects (elements of population)\n",
    "    '''\n",
    "    from neuronunit.optimization import nsga_parallel\n",
    "    DO.td = td\n",
    "    assert DO.td == td\n",
    "    return_package = nsga_parallel.update_pop(pop1,td);\n",
    "    dtc_pop = []\n",
    "    for i,r in enumerate(return_package):\n",
    "        dtc_pop.append(r[0])\n",
    "        dtc_pop[i].error = None\n",
    "        dtc_pop[i].error = np.sqrt(np.mean(np.square(list(pop1[i].fitness.values))))\n",
    "    sorted_list  = sorted([(dtc,dtc.error) for dtc in dtc_pop],key=lambda x:x[1])\n",
    "    dtc_pop = [dtc[0] for dtc in sorted_list]\n",
    "    print(dtc_pop,sorted_list)\n",
    "    return dtc_pop\n",
    "\n",
    "DO.td = td\n",
    "print(hof[0])\n",
    "print(len(hof))\n",
    "dtc_pop = pop2dtc(hof[0:-1],DO,td)\n",
    "\n",
    "miniga = dtc_pop[0].error\n",
    "maxiga = dtc_pop[-1].error\n",
    "maximaga = dtc_pop[-1]\n",
    "minimaga = dtc_pop[0]\n",
    "\n",
    "CACHE_PF = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if CACHE_PF == False:\n",
    "    h = list(history.genealogy_history.values())\n",
    "    evaluated_history = []\n",
    "    for i in h:\n",
    "        if hasattr(i,'rheobase'):\n",
    "            i.error = None\n",
    "            i.error = np.sqrt(np.mean(np.square(list(i.fitness.values))))\n",
    "            evaluated_history.append(i)\n",
    "    sorted_list  = sorted([(i,i.error) for i in evaluated_history ],key=lambda x:x[1])\n",
    "\n",
    "    with open('pf_dump.p','wb') as f:\n",
    "       pickle.dump([ sorted_list, evaluated_history ],f)\n",
    "else: \n",
    "     \n",
    "     unpack = pickle.load(open('pf_dump.p','rb'))\n",
    "     print(unpack)\n",
    "     sorted_list_pf = unpack[0]\n",
    "     pareto_dtc = unpack[1] \n",
    "\n",
    "minimaga_ind = sorted_list[0][0]\n",
    "maximaga_ind = sorted_list[-1][0]\n",
    "miniga = sorted_list[0][1]\n",
    "maxiga = sorted_list[-1][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(miniga)\n",
    "print(maxiga)\n",
    "print(minimaga_ind.fitness.values)\n",
    "print(maximaga_ind.fitness.values)\n",
    "print(len(minimaga_ind.fitness.values))\n",
    "print(dtcpopg[0][0].rheobase)\n",
    "print(dtcpopg[0][0].scores)\n",
    "print(sorted_list[-1][0].rheobase)\n",
    "print(sorted_list[-1][0].fitness.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def use_dtc_to_plotting(dtcpop,minimagr):\n",
    "    from neuronunit.capabilities import spike_functions\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    plt.clf()\n",
    "    plt.style.use('ggplot')\n",
    "    fig, axes = plt.subplots(figsize=(10, 10), facecolor='white')\n",
    "    stored_min = []\n",
    "    stored_max = []\n",
    "    for dtc in dtcpop[1:-1]:\n",
    "        plt.plot(dtc.tvec, dtc.vm0,linewidth=3.5, color='grey')\n",
    "        stored_min.append(np.min(dtc.vm0))\n",
    "        stored_max.append(np.max(dtc.vm0))\n",
    "        \n",
    "    from neuronunit.models.reduced import ReducedModel\n",
    "    from neuronunit.optimization.get_neab import tests as T\n",
    "    from neuronunit.optimization import get_neab\n",
    "    from neuronunit.optimization import evaluate_as_module\n",
    "    from neuronunit.optimization.evaluate_as_module import pre_format\n",
    "    model = ReducedModel(get_neab.LEMS_MODEL_PATH,name=str('vanilla'),backend='NEURON')\n",
    "    import neuron\n",
    "    model._backend.reset_neuron(neuron)\n",
    "    model.set_attrs(minimagr.attrs)\n",
    "    model.rheobase = minimagr.rheobase['value']\n",
    "    minimagr = pre_format(minimagr)\n",
    "    parameter_list = list(minimagr.vtest.values())\n",
    "    model.inject_square_current(parameter_list[0])\n",
    "    model._backend.local_run()\n",
    "    assert model.get_spike_count() == 1\n",
    "    print(model.get_spike_count(),bool(model.get_spike_count() == 1))\n",
    "    brute_best = list(model.results['vm'])\n",
    "\n",
    "    plt.plot(dtcpop[0].tvec, brute_best,linewidth=1, color='blue',label='best candidate via grid')#+str(mini.scores))\n",
    "    plt.plot(dtcpop[0].tvec,dtcpop[0].vm0,linewidth=1, color='red',label='best candidate via GA')#+str(miniga.scores))\n",
    "    plt.legend()\n",
    "    plt.ylabel('$V_{m}$ mV')\n",
    "    plt.xlabel('ms')\n",
    "    plt.show()\n",
    "from neuronunit import plottools\n",
    "from neuronunit.plottools import dtc_to_plotting\n",
    "\n",
    "CACHE_PLOTTING = False\n",
    "if CACHE_PLOTTING == False:\n",
    "    dtc_pop = dview.map_sync(dtc_to_plotting,dtc_pop )\n",
    "    with open('plotting_dump.p','wb') as f:\n",
    "       pickle.dump(dtc_pop,f)\n",
    "else: \n",
    "     dtc_pop  = pickle.load(open('plotting_dump.p','rb'))\n",
    "\n",
    "use_dtc_to_plotting(dtc_pop,minimagr_dtc)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment on plot\n",
    "There is good agreement between traces produced by the best candidate found by Genetic Algorithm, and exhaustive grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(dtc_pop[0].scores)\n",
    "print(minimagr_dtc.scores)\n",
    "print(sum(list(dtc_pop[0].scores.values())))\n",
    "print(sum(list(minimagr_dtc.scores.values())))\n",
    "miniga = sum(list(dtc_pop[0].scores.values()))\n",
    "print(miniga)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize distance between minimimum error and maximum error.\n",
    "This step will allow the GA's performance to be located within or below the range of error found by grid search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(maxi)\n",
    "print(mini)\n",
    "print(miniga)\n",
    "quantize_distance = list(np.linspace(mini,maxi,10))\n",
    "\n",
    "# check that the nsga error is in the bottom 1/5th of the entire error range.\n",
    "print('Report: ')\n",
    "print(\"Success\" if bool(miniga < quantize_distance[0]) else \"Failure\")\n",
    "print(\"The nsga error %f is in the bottom 1/5th of the entire error range\" % miniga)\n",
    "print(\"Minimum = %f; 20th percentile = %f; Maximum = %f\" % (mini,quantize_distance[0],maxi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below reports on the differences between between attributes of best models found via grid versus attributes of best models found via GA search:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from neuronunit.optimization import evaluate_as_module as eam\n",
    "NSGAO = NSGA(0.85)\n",
    "NSGAO.setnparams(nparams=nparams,provided_keys=provided_keys)\n",
    "#td = eam.get_trans_dict(NSGAO.subset)\n",
    "#print(td)\n",
    "td = { v:k for k,v in enumerate(td) }\n",
    "from neuronunit.optimization import model_parameters as modelp\n",
    "mp = modelp.model_params\n",
    "#minimaga = pareto_dtc[0]\n",
    "for k,v in minimagr_dtc.attrs.items():\n",
    "    #hvgrid = np.linspace(np.min(mp[k]),np.max(mp[k]),10)\n",
    "    dimension_length = np.max(mp[k]) - np.min(mp[k])\n",
    "    solution_distance_in_1D = np.abs(float(hof[0][td[k]]))-np.abs(float(v))\n",
    "        \n",
    "    #solution_distance_in_1D = np.abs(float(minimaga.attrs[k]))-np.abs(float(v))\n",
    "    relative_distance = dimension_length/solution_distance_in_1D\n",
    "    print('the difference between brute force candidates model parameters and the GA\\'s model parameters:')\n",
    "    print(float(hof[0][td[k]])-float(v),hof[0][td[k]],v,k)\n",
    "    print('the relative distance scaled by the length of the parameter dimension of interest:')\n",
    "    print(relative_distance)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print('the difference between the bf error and the GA\\'s error:')\n",
    "print('grid search:')\n",
    "from numpy import square, mean, sqrt\n",
    "rmsg = sqrt(mean(square(list(minimagr_dtc.scores.values()))))\n",
    "print(rmsg)\n",
    "print('ga:')\n",
    "rmsga = sqrt(mean(square(list(dtc_pop[0].scores.values()))))\n",
    "print(rmsga)\n",
    "print('Hall of Fame front')\n",
    "print(sqrt(mean(square(list(hof[0].fitness.values)))))\n",
    "print(miniga)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If any time is left over, may as well compute a more accurate grid, to better quantify GA performance in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neuronunit.optimization import get_neab\n",
    "#fi_basket = {'nlex_id':'NLXCELL:100201'}\n",
    "neuron = {'nlex_id': 'nifext_50'} \n",
    "\n",
    "error_criterion, inh_observations = get_neab.get_neuron_criteria(fi_basket)\n",
    "print(error_criterion)\n",
    "\n",
    "from bluepyopt.deapext.optimisations import DEAPOptimisation\n",
    "\n",
    "DO = DEAPOptimisation(error_criterion=error_criterion)\n",
    "DO.setnparams(nparams = nparams, provided_keys = provided_keys)\n",
    "pop, hof, log, history, td, gen_vs_hof = DO.run(offspring_size = MU, max_ngen = NGEN, cp_frequency=4,cp_filename='checkpointedGA.p')\n",
    "with open('ga_dump.p','wb') as f:\n",
    "   pickle.dump([pop, log, history, hof, td],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer V pyramidal cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
